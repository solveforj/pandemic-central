import pandas as pd
import numpy as np
import glob

weeks = [1, 2, 3, 4]
dates = ["2020-11-01", "2020-11-08", "2020-11-15", "2020-11-22", "2020-11-29","2020-12-06", "2020-12-13", "2020-12-20", "2020-12-27", "2021-01-03", "2021-01-10"]

def rt_stats():
    # Compute total number of counties for which forecasts were generated by the random forest model
    z = []
    for i in dates:
        q = list(pd.read_csv("output/ReichLabFormat/publication/{i}-PandemicCentral-COVIDForest.csv".format(i=i), dtype={'location':'str'})['location'].unique())
        z.append(q)
    elements_in_all = list(set.intersection(*map(set, z)))
    elements_in_all.sort()

    print("• Total forecasted counties by random forest")
    print(len(elements_in_all))

    # Compute Rt-related statistics
    higher_corrs = pd.read_csv("publication/data/higher_corrs.csv", dtype={'FIPS':'str'})
    higher_corrs = higher_corrs[higher_corrs['FIPS'].isin(elements_in_all)]
    population = pd.read_csv("data/census/census.csv", dtype={'FIPS':'str'}, usecols=['FIPS', 'POP_DENSITY'])
    higher_corrs = pd.merge(left=higher_corrs, right=population, how='left', on=['FIPS'], copy=False)

    print("• Average shift")
    print(higher_corrs['shift'].mean())
    state_higher_corrs = higher_corrs[higher_corrs['region'] == 'state']
    print("• Number of counties with higher state max correlation + their mean shift + their mean population density")
    print(len(state_higher_corrs), state_higher_corrs['shift'].mean(), state_higher_corrs['POP_DENSITY'].mean())
    county_higher_corrs = higher_corrs[higher_corrs['region'] == 'county']
    print("• Number of counties with higher county max correlation + their mean shift")
    print(len(county_higher_corrs), county_higher_corrs['shift'].mean())
    print("• Number of counties with max Pearson correlation < 0.50 (between cases and aligned Rt)")
    print(len(higher_corrs[higher_corrs['correlation'] < 0.5]))
    print("  • Average population density of the counties above")
    print(higher_corrs[higher_corrs['correlation'] < 0.5]['POP_DENSITY'].mean())
    print("• Average population density of all counties")
    print(higher_corrs['POP_DENSITY'].mean())
    print("\n")

def model_R2_MAE_stats():

    files = glob.glob("output/model_stats/publication/*.csv")

    final_df = pd.DataFrame()

    data = pd.read_csv("data/JHU/jhu_data.csv")
    data = data[['FIPS', 'date', 'confirmed_cases']]
    data = data.rename({'confirmed_cases':'weekly_sum'}, axis=1)

    data['shift'] = [-7] * len(data)
    data['shift'] = pd.to_timedelta(data['shift'], unit='D')
    data['shift_date'] = pd.to_datetime(data['date']) + data['shift']
    data['shift_date'] = data['shift_date'].astype(str)
    data = data.drop(['shift'], axis=1)[['FIPS', 'shift_date', 'weekly_sum']]
    data = data.rename({'shift_date': 'date'}, axis=1).reset_index(drop=True)

    populations = pd.read_csv("data/census/census.csv", usecols=['FIPS', 'TOT_POP'])
    jhu = pd.merge(left=data, right=populations, how='left', on=['FIPS'], copy=False)

    for i in files:
        df = pd.read_csv(i)
        df = df[df['model_type'] == "mobility"]
        final_df = pd.concat([final_df, df[['week', 'MAE_testing', 'MAE_training','R2_testing', 'R2_training']]], axis=0)

        graph_df = df[['week', 'MAE_testing']].reset_index(drop=True)
        graph_df['data_type'] = graph_df['week'].apply(lambda x : "week_" + str(x) + "_mae").reset_index(drop=True)
        graph_df = graph_df.drop(['week'], axis=1)
        graph_df = graph_df.rename(columns={'MAE_testing': 'cases_per_100k'})
        week_name = i.split("_")[-1].split(".")[0]

        jhu_week = jhu[jhu['date'] == week_name]
        tot_cases = jhu_week['weekly_sum'].sum()
        tot_pop = jhu_week['TOT_POP'].sum()

    final_df_ = final_df.groupby("week").agg('mean').round(2)
    final_df_ = final_df_.astype(str)
    final_std = final_df.groupby("week").agg('std').round(2)
    final_std = final_std.astype(str)

    final_df_['MAE_testing'] += "±" + final_std['MAE_testing']
    final_df_['MAE_training'] += "±" + final_std['MAE_training']
    final_df_['R2_testing'] += "±" + final_std['R2_testing']
    final_df_['R2_training'] += "±" + final_std['R2_training']

    print("• Average MAE + R2 for training and validation datasets over 11 weeks")
    print(final_df_)
    print("\n")

def case_increase_nov_dec():
    jhu_data = pd.read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv").dropna()
    jhu_data['FIPS'] = jhu_data['FIPS'].astype(int).astype(str)

    jhu_data = jhu_data[jhu_data['FIPS'].notnull()]

    def process_FIPS(fips):
        missing_zeroes = "0" * (5-len(fips))
        return missing_zeroes + fips

    jhu_data['FIPS'] = jhu_data['FIPS'].apply(lambda x : process_FIPS(x))

    # Filter for non-errant counties
    fips_to_use = pd.read_csv("data/geodata/FIPS_used.csv", dtype={'FIPS': 'str'})
    jhu_data = jhu_data[jhu_data['FIPS'].isin(fips_to_use['FIPS'].to_list())]

    jhu_data = jhu_data.drop(["Admin2","Province_State","Country_Region","Lat","Long_","Combined_Key","UID","iso2","iso3","code3"], axis=1)
    jhu_data = jhu_data.melt(id_vars=['FIPS'], var_name = 'date', value_name = 'confirmed_cases')
    jhu_data['date'] = pd.to_datetime(jhu_data['date'])
    jhu_data = jhu_data.sort_values(['FIPS', 'date'])

    # Case counts are cumulative and will be converted into daily change
    jhu_data['confirmed_cases'] = jhu_data.groupby('FIPS')['confirmed_cases'].diff().dropna()
    cases = jhu_data
    cases = cases.sort_values(['FIPS','date'], axis=0)
    cases = cases.reset_index(drop=True)
    cases['date'] = cases['date'].astype(str)

    cases_nov_wk1 = cases[(cases['date'] >= "2020-11-01") & (cases['date'] <= "2020-11-07")]
    cases_nov_wk4 = cases[(cases['date'] >= "2020-11-24") & (cases['date'] <= "2020-11-30")]
    cases_nov_wk1 = cases_nov_wk1['confirmed_cases'].sum()
    cases_nov_wk4 = cases_nov_wk4['confirmed_cases'].sum()
    print("• Cases %Change from first to last week of November 2020")
    print((cases_nov_wk4 - cases_nov_wk1) / cases_nov_wk1)
    print("\n")

    cases_dec_wk1 = cases[(cases['date'] >= "2020-12-01") & (cases['date'] <= "2020-12-07")]
    cases_dec_wk4 = cases[(cases['date'] >= "2020-12-25") & (cases['date'] <= "2020-12-31")]
    cases_dec_wk1 = cases_dec_wk1['confirmed_cases'].sum()
    cases_dec_wk4 = cases_dec_wk4['confirmed_cases'].sum()
    print("• Cases %Change from first to last week of December 2020")
    print((cases_dec_wk4 - cases_dec_wk1) / cases_dec_wk1)
    print("\n")

def top_feature_importance_share():
    unfiltered_percents = []
    len_unfiltered = 0
    filter_list = ['prediction_aligned_int_7', 'prediction_aligned_int_14','prediction_aligned_int_21', 'prediction_aligned_int_28', \
                    'rt_aligned_int_7', 'rt_aligned_int_14', 'rt_aligned_int_21', 'rt_aligned_int_28', \
                    'test_positivity', 'totalTestResultsIncrease_norm']

    for i in weeks:
        ranking_dfs = []
        for j in dates:
            df_path = "output/feature_ranking/publication/franking_" + str(i) + "_" +  j + ".csv"
            read_df = pd.read_csv(df_path, index_col=0)
            ranking_dfs.append(read_df)
        ranking_df = pd.concat(ranking_dfs, axis=1)
        ranking_df.columns = range(ranking_df.shape[1])
        ranking_df['sum'] = ranking_df.sum(axis=1)
        total = ranking_df['sum'].sum()
        filtered_df = ranking_df[ranking_df.index.isin(filter_list)]
        len_unfiltered = len(ranking_df) - len(filtered_df)
        filtered_total = filtered_df['sum'].sum()
        unfiltered_percents.append((total-filtered_total)/total)

    print("• % of total feature permutation importance not for top 4 features + the # of these features")
    print(np.mean(unfiltered_percents), len_unfiltered)
    print("\n")

def misc_stats():
    print("DISPLAYING MISCELLANEOUS STATISTICS IN MANUSCRIPT\n")
    rt_stats()
    model_R2_MAE_stats()
    case_increase_nov_dec()
    top_feature_importance_share()
